{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE421_Lab_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c58PTl8gjBK"
      },
      "source": [
        "# Unsupervised Learning and Probabilistic Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2TyoRRT4E2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13fba46-5456-4a58-dd07-2cb85ce48d71"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yut8oVwlUl1s"
      },
      "source": [
        "def reduce_logsumexp(input_tensor, reduction_indices=1, keep_dims=False):\n",
        "  \"\"\"Computes the sum of elements across dimensions of a tensor in log domain.\n",
        "\n",
        "     It uses a similar API to tf.reduce_sum.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: The tensor to reduce. Should have numeric type.\n",
        "    reduction_indices: The dimensions to reduce. \n",
        "    keep_dims: If true, retains reduced dimensions with length 1.\n",
        "  Returns:\n",
        "    The reduced tensor.\n",
        "  \"\"\"\n",
        "  max_input_tensor1 = tf.reduce_max(\n",
        "      input_tensor, reduction_indices, keep_dims=keep_dims)\n",
        "  max_input_tensor2 = max_input_tensor1\n",
        "  if not keep_dims:\n",
        "    max_input_tensor2 = tf.expand_dims(max_input_tensor2, reduction_indices)\n",
        "  return tf.log(\n",
        "      tf.reduce_sum(\n",
        "          tf.exp(input_tensor - max_input_tensor2),\n",
        "          reduction_indices,\n",
        "          keep_dims=keep_dims)) + max_input_tensor1\n",
        "\n",
        "\n",
        "def logsoftmax(input_tensor):\n",
        "  \"\"\"Computes normal softmax nonlinearity in log domain.\n",
        "\n",
        "     It can be used to normalize log probability.\n",
        "     The softmax is always computed along the second dimension of the input Tensor.     \n",
        "\n",
        "  Args:\n",
        "    input_tensor: Unnormalized log probability.\n",
        "  Returns:\n",
        "    normalized log probability.\n",
        "  \"\"\"\n",
        "  return input_tensor - reduce_logsumexp(input_tensor, reduction_indices=0, keep_dims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7SkTRNpU6Ok"
      },
      "source": [
        "#K-means\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKTvH3-aU9Mr"
      },
      "source": [
        "## Learning K-means\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8a03uuIUjMp"
      },
      "source": [
        "# Distance function for K-means\n",
        "def distanceFunc(X, MU):\n",
        "    # Inputs\n",
        "    # X: is an NxD matrix (N observations and D dimensions)\n",
        "    # MU: is an KxD matrix (K means and D dimensions)\n",
        "    # Outputs\n",
        "    # pair_dist: is the pairwise distance matrix (NxK)\n",
        "\n",
        "    X_expand = tf.expand_dims(X, 0)\n",
        "    MU_expand = tf.expand_dims(MU, 1)\n",
        "\n",
        "    sqr_distance = tf.square(tf.subtract(X_expand, MU_expand))\n",
        "    sqr_distance = tf.reduce_sum(sqr_distance, axis = 2)\n",
        "    sqr_distance = tf.transpose(sqr_distance)\n",
        "\n",
        "    return sqr_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK5j_kIY5-N4"
      },
      "source": [
        "def buildGraphKM(data, data_dim, K, lr):\n",
        "\n",
        "  data_points = tf.placeholder(tf.float32, (None, data_dim), name = \"data\")\n",
        "\n",
        "  # Initialize the cluster centers based on a random sample of\n",
        "  # data points in the set...\n",
        "  centers = tf.Variable(tf.cast(tf.slice(tf.random_shuffle(data), [0, 0], (K, -1)), dtype=tf.float32), dtype=tf.float32)\n",
        "\n",
        "  # Out loss function is the pairwise distance between points and \n",
        "  # their cluster centers...\n",
        "  loss = tf.reduce_sum(tf.reduce_min(distanceFunc(data_points, centers), axis = 1))\n",
        "\n",
        "  optim = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.99, epsilon=1e-5)\n",
        "  optim = optim.minimize(loss)\n",
        "\n",
        "  return data_points, centers, optim, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqOCt9nGEPAP"
      },
      "source": [
        "def plotScatter(title, sample_points, centers, K, size, assign, min_val_loss):\n",
        "  frequency = np.bincount(assign)\n",
        "  index = np.nonzero(frequency)[0]\n",
        "  frequency = zip(index, frequency[index])\n",
        "\n",
        "  iter = 1\n",
        "  percentages = []\n",
        "  for i in frequency:\n",
        "    percentages.append(\"Cluster \" + str(i[0] + 1) + \" \" + str(round((i[1] / len(assign)) * 100, 2)) +  \"%\")\n",
        "    iter += 1\n",
        "\n",
        "  plt.title(title)\n",
        "  \n",
        "  for i in range(K):\n",
        "    plt.scatter(sample_points[i][:, 0], sample_points[i][:, 1], alpha=1, s=5, label = percentages[i])\n",
        "\n",
        "  plt.scatter(centers[:, 0], centers[:, 1], marker='x', s = 50, c='black')\n",
        "\n",
        "  plt.text(0, -6.5, \"Validation loss: \" + str(min_val_loss), ha='center')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "269oIsLWH_Ly"
      },
      "source": [
        "def sampleColours(sample_points, centers):\n",
        "  # For each sample find the closest center and then assign it a colour...\n",
        "  closest = tf.arg_min(distanceFunc(sample_points, centers), 1)\n",
        "  return closest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_r3oVGm-wMW"
      },
      "source": [
        "def K_means(K, lr, is_valid = False, epochs=10, plot=True, npy=2):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "\n",
        "  # Loading data\n",
        "  if (npy == 2):\n",
        "    data = np.load('data2D.npy')\n",
        "  else:\n",
        "    data = np.load('data100D.npy')\n",
        "  [num_pts, dim] = np.shape(data)\n",
        "\n",
        "  # For Validation set\n",
        "  if is_valid:\n",
        "    valid_batch = int(num_pts / 3.0)\n",
        "    np.random.seed(45689)\n",
        "    rnd_idx = np.arange(num_pts)\n",
        "    np.random.shuffle(rnd_idx)\n",
        "    val_data = data[rnd_idx[:valid_batch]]\n",
        "    data = data[rnd_idx[valid_batch:]]\n",
        "  \n",
        "  X, MU, optim, loss = buildGraphKM(data, dim, K, lr)\n",
        "\n",
        "  init_op = tf.global_variables_initializer()\n",
        "  tf.set_random_seed(1000)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    print(\"Starting K-means...\")\n",
        "\n",
        "    feed = {\n",
        "      X : data\n",
        "    }\n",
        "\n",
        "    for itteration in range(epochs):\n",
        "      centers, losses, _ = sess.run([MU, loss, optim], feed_dict = feed)\n",
        "\n",
        "      train_loss.append(losses / len(data))\n",
        "\n",
        "      # Calculate the validation loss\n",
        "      if is_valid:\n",
        "        valid_center, valid_loss, _ = sess.run([MU, loss, optim], feed_dict  = {X : val_data})\n",
        "        val_loss.append(valid_loss / len(val_data))\n",
        "\n",
        "\n",
        "    colours = sess.run(sampleColours(data, centers), feed_dict = {X: data, MU : centers})\n",
        "  \n",
        "  cluster_data = []\n",
        "\n",
        "  for i in range(K):\n",
        "    cluster_data.append(data[colours == i])\n",
        "\n",
        "  if (plot):\n",
        "    min_loss = round(val_loss[np.argmin(val_loss)], 4)\n",
        "\n",
        "    plotScatter(\"K-means Clustering\", cluster_data, centers, K, len(data), colours, min_loss)\n",
        "\n",
        "  plt.title(\"K-means loss\")\n",
        "  plt.plot(train_loss, label=\"Training loss\")\n",
        "  plt.plot(val_loss, label=\"Validation loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  return train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG-ZJDByApT4"
      },
      "source": [
        "num_clusters = [1, 2, 3, 4, 5]\n",
        "\n",
        "for k in num_clusters:\n",
        "  train_loss, val_loss = K_means(k, 0.1, is_valid=True, epochs=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxseJBdkgPgc"
      },
      "source": [
        "# Mixture of Gaussians"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5szsYFgUPT"
      },
      "source": [
        "## The Gaussian cluster model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGc_sWpOBggm"
      },
      "source": [
        "$\n",
        "P(x | z) = \\frac{P(z | x)P(x)}{P(z)}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCQLftNyUXoF"
      },
      "source": [
        "# Distance function for GMM\n",
        "def distanceFunc(X, MU):\n",
        "    # Inputs\n",
        "    # X: is an NxD matrix (N observations and D dimensions)\n",
        "    # MU: is an KxD matrix (K means and D dimensions)\n",
        "    # Outputs\n",
        "    # pair_dist: is the pairwise distance matrix (NxK)\n",
        "\n",
        "    X_expand = tf.expand_dims(X, 0)\n",
        "    MU_expand = tf.expand_dims(MU, 1)\n",
        "\n",
        "    sqr_distance = tf.square(tf.subtract(X_expand, MU_expand))\n",
        "    sqr_distance = tf.reduce_sum(sqr_distance, axis = 2)\n",
        "    sqr_distance = tf.transpose(sqr_distance)\n",
        "\n",
        "    return sqr_distance\n",
        "\n",
        "def log_GaussPDF(X, mu, sigma):\n",
        "    # Inputs\n",
        "    # X: N X D\n",
        "    # mu: K X D\n",
        "    # sigma: K X 1\n",
        "\n",
        "    # Outputs:\n",
        "    # log Gaussian PDF N X K\n",
        "\n",
        "    # Recall pdf of gaussian:\n",
        "    # pdf(x; mu, sigma) = exp(-0.5 (x - mu)**2 / sigma**2) / Z\n",
        "    # Z = (2 pi sigma**2)**0.5\n",
        "    # log(pdf) = (-0.5 (x - mu)**2 / sigma ** 2) - log(Z)\n",
        "\n",
        "    data_dim = tf.cast(X.shape[1], tf.float32)\n",
        "    Z = (2 * np.pi * tf.square(sigma))\n",
        "    Z = (data_dim/2) * tf.log(Z)\n",
        "    pdf = -0.5 * tf.divide(distanceFunc(X, mu), tf.squeeze(tf.square(sigma)))\n",
        "\n",
        "    log_gauss = pdf - tf.transpose(Z)\n",
        "    return log_gauss \n",
        "\n",
        "def log_posterior(log_PDF, log_pi):\n",
        "    # Input\n",
        "    # log_PDF: log Gaussian PDF N X K\n",
        "    # log_pi: K X 1\n",
        "\n",
        "    # Outputs\n",
        "    # log_post: N X K\n",
        "\n",
        "    # log[P(z | x)] = log[P(x | z)] + log[P(z)] - \n",
        "    #                 log[Sum_1_N{exp(log[P(x | z)] + log[P(z)])}] \n",
        "\n",
        "    log_post_num = tf.add(log_PDF, tf.transpose(log_pi)) \n",
        "    log_post_dem = reduce_logsumexp(log_PDF + log_pi, keep_dims=True)\n",
        "\n",
        "    return log_post_num - log_post_dem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZQZ7OYTNUZK"
      },
      "source": [
        "## Learning the MoG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37J85sF56UEh"
      },
      "source": [
        "def buildGraphGMM(data, data_dim, K, lr, div):\n",
        "\n",
        "  data_points = tf.placeholder(tf.float32, (None, data_dim), name = \"data\")\n",
        "\n",
        "  # Initialize the cluster centers based on a random sample of\n",
        "  # data points in the set..\n",
        "  centers = tf.Variable(tf.cast(tf.slice(tf.random_shuffle(data), [0, 0], (K, -1)), dtype=tf.float32), dtype=tf.float32) \n",
        "\n",
        "  # Initialize the standard deviation of each function to be a random\n",
        "  # sample from a gaussian distribution\n",
        "  # Make it exponential to deal with constraint...\n",
        "  sigma = tf.Variable(tf.random_normal((K, 1), stddev= div), dtype=tf.float32)\n",
        "  sigma = tf.exp(sigma)\n",
        "\n",
        "  # The weights of each distribution \n",
        "  # But again because we're using exp sigma, we need to change \n",
        "  # pi_k to be represented by a softmax function\n",
        "  pi = tf.Variable(tf.random_normal((K, 1), stddev= div), dtype=tf.float32)\n",
        "  pi = tf.squeeze(logsoftmax(pi))\n",
        "\n",
        "  # Loss = -log[P(X)]\n",
        "  #      = -log[π_1_N{P(x_n)}]\n",
        "  #      = -log[π_1_N{Σ_1_K{π_k N(x_n)}}]\n",
        "  #      = -log[π_1_N{Σ_1_K{e ^ (log[π_k] + log[N(x_n)])}}]\n",
        "  #      = -log[Σ_1_K{e ^ (log[π_k] + log[N(x_1)])}] - ... \n",
        "  #        -log[Σ_1_K{e ^ (log[π_k] + log[N(x_N)])}]\n",
        "  log_pdf = log_GaussPDF(data_points, centers, sigma)\n",
        "  loss = reduce_logsumexp(log_pdf + pi, 1, keep_dims=True)\n",
        "  loss = -1 * tf.reduce_sum(loss)\n",
        "\n",
        "  optim = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.99, epsilon=1e-5)\n",
        "  optim = optim.minimize(loss)\n",
        "\n",
        "  # Based on the distributions make a prediction as to which \n",
        "  # cluster a data point belongs too\n",
        "  classify = log_posterior(log_pdf, pi)\n",
        "  classify = tf.nn.softmax(classify)\n",
        "  classify = tf.argmax(classify, axis = 1)\n",
        "\n",
        "  return data_points, centers, sigma, pi, optim, loss, classify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV0LCjUvS-a2"
      },
      "source": [
        "def GMM(K, lr, stddve, is_valid = False, epochs=10, plot=True, npy=1):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  train_cluster = []\n",
        "\n",
        "  # Loading data\n",
        "  if (npy == 2):\n",
        "    data = np.load('data2D.npy')\n",
        "  else:\n",
        "    data = np.load('data100D.npy')\n",
        "  \n",
        "  [num_pts, dim] = np.shape(data)\n",
        "\n",
        "  # For Validation set\n",
        "  if is_valid:\n",
        "    valid_batch = int(num_pts / 3.0)\n",
        "    np.random.seed(45689)\n",
        "    rnd_idx = np.arange(num_pts)\n",
        "    np.random.shuffle(rnd_idx)\n",
        "    val_data = data[rnd_idx[:valid_batch]]\n",
        "    data = data[rnd_idx[valid_batch:]]\n",
        "  \n",
        "  X, MU, sigma, pi, optim, loss, classify = buildGraphGMM(data, dim, K, lr, stddve)\n",
        "\n",
        "  init_op = tf.global_variables_initializer()\n",
        "  tf.set_random_seed(1000)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    print(\"Starting GMM...\")\n",
        "\n",
        "    feed = {\n",
        "      X : data\n",
        "    }\n",
        "\n",
        "    for itteration in range(epochs):\n",
        "      centers, losses, cluster_assign, _ = sess.run([MU, loss, classify, optim], feed_dict = feed)\n",
        "\n",
        "      train_loss.append(losses / len(data))\n",
        "      train_cluster.append(cluster_assign)\n",
        "\n",
        "      if is_valid:\n",
        "        valid_center, valid_loss, _c, _o = sess.run([MU, loss, classify, optim], feed_dict = {X : val_data})\n",
        "        val_loss.append(valid_loss / len(val_data))\n",
        "    \n",
        "    # Create scatter plot\n",
        "    cluster_data = []\n",
        "    \n",
        "    for i in range(K):\n",
        "      cluster_data.append(data[cluster_assign == i])\n",
        "\n",
        "    # print(\"Trained parameters:\", sess.run(sigma), sess.run(pi), centers)\n",
        "\n",
        "  if (plot):  \n",
        "    min_loss = round(val_loss[np.argmin(val_loss)], 4)\n",
        "\n",
        "    plotScatter(\"GMM Clustering\", cluster_data, centers, K, len(data), cluster_assign, min_loss)\n",
        "\n",
        "  plt.title(\"GMM loss\")\n",
        "  plt.plot(train_loss, label=\"Training loss\")\n",
        "  plt.plot(val_loss, label=\"Validation loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrHwt2O0yqY9"
      },
      "source": [
        "Best learned parameters with K = 3 <br>\n",
        "$\\sigma :$ 0.9934453 ,0.1977245 <br>\n",
        "$\\pi :$ -1.094792, -1.1026771, -1.0983831 <br>\n",
        "$\\mu :$ [ 0.10631254, -1.5272143 ], [-1.1020751,  -3.306512  ], [ 1.2982769,   0.30938783] <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOF-9_GsT712"
      },
      "source": [
        "train_loss, val_loss = GMM(3, 0.01, 0.05, True, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygx8y-_9zTc0"
      },
      "source": [
        "num_clusters = [1, 2, 3, 4, 5]\n",
        "\n",
        "for k in num_clusters:\n",
        "  train_loss, val_loss = GMM(k, 0.01, 0.05, True, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTI8IquK3Fij"
      },
      "source": [
        "num_clusters = [5, 10, 15, 20, 30]\n",
        "\n",
        "for k in num_clusters:\n",
        "  train_loss, val_loss = GMM(k, 0.01, 0.05, True, 200, plot=False, npy=100)\n",
        "  print(\"Training loss:\", train_loss[np.argmin(train_loss)])\n",
        "  print(\"Validation loss:\", val_loss[np.argmin(val_loss)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo_FilHs6YIS"
      },
      "source": [
        "loss_over_time = [85.4696407140714, 48.51731266876688, 48.28689431443144, 47.97822438493849, 47.87642983048305]\n",
        "K = [5, 10, 15, 20, 30]\n",
        "\n",
        "plt.plot(K, loss_over_time)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiUbyZY246C1"
      },
      "source": [
        "num_clusters = [5, 10, 15, 20, 30]\n",
        "\n",
        "for k in num_clusters:\n",
        "  train_loss, val_loss = K_means(k, 0.01, True, 200, plot=False, npy=100)\n",
        "  print(\"Training loss:\", train_loss[np.argmin(train_loss)])\n",
        "  print(\"Validation loss:\", val_loss[np.argmin(val_loss)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U10sOEkc69QN"
      },
      "source": [
        "loss_over_time = [21.659822232223224, 20.872030171767175, 20.534962871287128, 20.391445394539453, 20.010754200420042]\n",
        "K = [5, 10, 15, 20, 30]\n",
        "\n",
        "plt.plot(K, loss_over_time)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}